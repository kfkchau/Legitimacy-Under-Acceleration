## DRAFT - Why Todayâ€™s AI Governance Still Feels Like Theatre to the People Running the Systems  

From the outside, it looks like weâ€™ve made real progress.

We have:  

- **UNESCO** and **OECD** AI principles  
- the **NIST AI Risk Management Framework**  
- the **EU AI Act**  
- **ISO/IEC 42001**  
- and thick binders of corporate â€œResponsible AIâ€ playbooks  

On paper, itâ€™s a lot.

But if you talk to the people actually **building and operating AI systems**, you hear something different:

> â€œWeâ€™ve got beautiful principles and terrible plumbing.â€

This piece is for them.

Iâ€™m not arguing that global frameworks are useless. They raised the floor. They gave us shared language, legitimacy and political cover to say â€œAI governance mattersâ€.

The problem is architectural:  

Weâ€™ve treated governance as **statements around the system**, not **machinery inside the system**.

From an operational point of view, five things keep breaking.

---

## 1. Beautiful principles, terrible plumbing

In most organisations today, governance looks like this:

- ethics principles on a slide,  
- a risk framework in Confluence,  
- a model evaluation checklist,  
- a â€œhuman-in-the-loopâ€ line in the design doc.

Then you ship.

When the real system misbehaves in production, none of those artefacts tell you:

- how the workflow was actually wired  
- who could call which function, against which rules  
- where the system should have been stopped  
- or how to replay what happened without a week of forensics

On the policy side, UNESCO/OECD/NIST/ISO/EU all make the same move:

- they define **what good AI should feel like** â€“ fair, transparent, accountable, human-centred  
- they define **obligations** for providers and deployers  
- they define **processes** â€“ risk assessments, monitoring, documentation  

But they mostly stop at the boundary of *â€œthe AI systemâ€* as a black box.

Inside the organisation, that lands as:

- â€œweâ€™re compliant on paperâ€  
- â€œâ€¦but our pipelines and workflows still run on vibesâ€

**Whatâ€™s missing is very simple to describe and very hard to build:**

> Governance that shows up in the wiring:  
> **who can do what, when, with what logs, and who can stop it.**

Until that exists, most governance will feel like theatre to the people in the engine room.

---

## 2. We donâ€™t have a clean map of who should do what

Every framework says some version of:

- â€œthere must be meaningful human oversightâ€

In practice, â€œhuman oversightâ€ usually means:

- somewhere in the slide deck, thereâ€™s a bullet saying â€œa human is in the loopâ€  
- nobody can agree which **decisions** are actually human  
- nobody can show, in a crisp trail, **why** a particular task was given to AI rather than a person

If you never draw a **task-level map**, â€œoversightâ€ collapses into a slogan.

From an operational point of view, we need to start from **tasks**, not job titles.

At minimum:

- break work down into **small decisions**:
  - one input,  
  - one transformation,  
  - one output  
- classify them roughly by **reasoning depth**:
  - *Why* â€“ purpose, ethics, improvisation  
  - *How* â€“ method, process, optimisation  
  - *What* â€“ concrete execution  
- and by **information messiness**:
  - structured, semi-structured, unstructured

Then you can say, with a straight face:

- these **Why-tier, high-ambiguity, high-consequence** tasks: human-only  
- these **structured, What-tier** tasks: safe to automate under guardrails  
- these in the middle: AI-assist, with defined checks and explicit logs

In my own work I call this a **task boundary framework** â€“ a reasoning-first way to decide:

> who should do what, under which conditions, with what trail.

You donâ€™t need my label. But without some shared task map, â€œhuman oversightâ€ will keep living as a sentence in a PDF, not a property of the system.

---

## 3. Our AI â€œteamâ€ has no constitution

Most organisations are now edging toward **multi-agent** or multi-tool setups, whether they call them that or not:

- a â€œcopilotâ€ that talks to users  
- a chain that calls internal tools  
- retrieval and summarisation layers  
- some monitoring around the edges

But the default pattern is still:

> one **mega-assistant** doing everything:
> - talks to the user  
> - searches  
> - plans  
> - writes  
> - calls tools  
> - self-checks  

No clear roles. No separation of powers. No governance layer with real veto.

Itâ€™s like putting a single very smart person in a sealed room with root access and saying â€œbe responsibleâ€.

If you were designing a critical system anywhere else â€“ aircraft, ships, power plants â€“ you would not accept that.

Youâ€™d ask:

- where is the **redundancy**?  
- where is the **voting**?  
- where is the **independent safety function** that can cut power if things go weird?

Aircraft donâ€™t rely on one perfect computer. They fly with **triplex redundancy**:

- three computers running in parallel,  
- comparing outputs,  
- cutting out the outlier when they disagree.

A shipâ€™s engine is judged less by â€œcan it hit this speed once?â€ and more by:

- can it run near max output for **hours** or **days**,  
- self-monitoring,  
- with humans able to intervene,  
- without killing everyone if something misfires.

We keep evaluating AI like students in an exam: *can it pass this one test?*  

In operations, we should care about AI systems the way we care about engines and operating systems:

- not â€œcan it boot once without errors?â€,  
- but â€œcan it run for weeks, self-monitor, and fail safely under load?â€

Right now, the **global and national AI governance stack** behaves more like a **patch-heavy consumer OS**:

- every new risk adds another guideline, another national layer, another exception  
- helpful in the moment,  
- but not designed as a coherent, long-running substrate

What we actually need is the **server-grade version of governance**:

- a small set of shared primitives â€“ tasks, roles, rights, logs, escalation paths â€“  
- that different jurisdictions and sectors can implement and extend,  
- while still inter-operating like parts of one operating system rather than a pile of loosely compatible patches.

For AI, we often do the opposite of what we do in real safety-critical systems:

- we obsess over **one-shot benchmarks** (â€œdid it pass this exam?â€)  
- but we donâ€™t design for **long-running, self-correcting, fail-safe operation**

An operations-centric governance architecture would look more like:

- a **team of small agents** with narrow, auditable roles:
  - one for extracting facts,  
  - one for checking policy fit,  
  - one for drafting options,  
  - one for user comms,  
  - one for tool access  
- plus a **governance layer**:
  - agents or services whose *only* job is to:
    - enforce boundaries,  
    - watch for risky patterns,  
    - and control starting/stopping and access â€“ even if they never see the content.

With simple rules like:

- if certain governance agents flag a risk, **the whole workflow pauses**  
- if a key â€œleadâ€ goes down, **a backup cohort** takes over or the human owner must intervene  
- agents cannot be muted or quietly bypassed; only the human owner can override them

Iâ€™ve been prototyping this as a governance-first â€œwhite-collar AI teamâ€ architecture, but the deeper point is simple:

> If youâ€™re going to let AI behave like a team,  
> you need the equivalent of a **constitution**, not a friendly monarch.

Right now, most AI â€œteamsâ€ donâ€™t have one.

---

## 4. When things go wrong, we keep having the same meetings

Ask anyone who has sat through enough AI or system incidents and theyâ€™ll recognise this pattern:

1. Something goes wrong.  
2. Thereâ€™s a post-mortem.  
3. A list of â€œlessons learnedâ€ and action items is produced.  
4. Six months later, a very similar incident happens.  
5. Repeat.

On paper, every framework tells you to:

- â€œmonitor and reviewâ€,  
- â€œcontinuously improveâ€,  
- â€œupdate your risk assessmentâ€.

In reality, **rule intake outpaces rule metabolism**:

- new policies, controls and insights accumulate,  
- very few rules get retired or fundamentally reworked,  
- nobody is clearly mandated to say:
  > â€œthis rule, in this scope, is now wrong.â€

We donâ€™t have a **rule-change engine**. We have meetings.

Operationally, that shows up as:

- known problems that never quite get fixed in the rules  
- â€œtemporary workaroundsâ€ that become permanent  
- controls that pile up without a concept of deprecation or retirement  
- the same failure mode showing up under different ticket numbers

A minimal rule-change engine doesnâ€™t require new philosophy. It needs:

- **triggers** â€“ events that *force* a review:
  - repeated incidents of a certain type,  
  - specific drift in metrics,  
  - new evidence about harm  
- **authority** â€“ clarity on:
  - who can declare â€œthis rule is brokenâ€ at team / product / org / cross-org level  
- **states** â€“ rules arenâ€™t just â€œthereâ€:
  - proposed, active, deprecated, retired  
- **trails** â€“ a way to see:
  - when and why a rule changed,  
  - who approved it,  
  - what evidence they relied on

In my own work I call this a **meta-governance kernel** â€“ a small set of rules about how rules change.

You donâ€™t need the spec to get the point:

> Until you treat rule-change as a **designed process**,  
> â€œcontinuous improvementâ€ will stay stuck at the slideware layer.

---

## 5. Even when it â€œworksâ€, people feel their work and worth dissolving

Most AI governance conversations live on the **rule side**:

- risk categories,  
- rights,  
- controls,  
- audits,  
- penalties.

Much less attention goes to the **value side**:

- which work still â€œcountsâ€  
- whose contribution is visible in metrics  
- what paths through the system are still open to people

I use a city image for this.

- The **traffic system** â€“ laws, lights, signs, enforcement â€“ is the **rules**.  
- The **routes people actually drive** â€“ where they go, which roads crowd, which areas they avoid â€“ are the **value signals**.

AI is hitting both sides at once.

On the rule side:

- we add principles, controls, obligations.

On the value side:

- AI takes over work that people used to rely on to answer:
  - â€œwhat am I good at?â€  
  - â€œwhat do people need from me?â€  
  - â€œwhat is a respectable life path here?â€  
- some forms of effort vanish from dashboards and KPIs  
- whole neighbourhoods of work become:
  - lower-status,  
  - lower-paid,  
  - or structurally precarious

You can be fully compliant with the EU AI Act and your internal RAIO policies, and still end up with:

- a workforce that feels hollowed out,  
- fewer meaningful paths for humans,  
- more invisible care work and glue work with no recognition.

Ethical **value assertion** (â€œwe respect human dignity and autonomyâ€) does not, by itself, produce **ethical outcomes**:

- if you donâ€™t change how decisions are structured,  
- if you donâ€™t change how contribution is recognised,  
- if you donâ€™t give people real alternatives when old roles erode.

A serious governance conversation has to see **both**:

- the traffic system (rules, enforcement, audits)  
- the routes (what journeys through work and life remain viable and meaningful)

Iâ€™m developing this further as a **governanceâ€“economic system** lens â€“ how rules and value signals interact under acceleration â€“ but the immediate point is simple:

> Donâ€™t declare AI governance â€œdoneâ€  
> until youâ€™ve looked at what your systems are doing to peopleâ€™s **future paths**, not just to todayâ€™s risk profile.

---

## 6. From governance as decoration to governance as infrastructure

None of this is an argument against UNESCO, OECD, NIST, the EU AI Act or ISO.

They did important first-generation work:

- made AI governance **speakable**  
- anchored it in human rights language  
- created hooks for enforcement and accountability  
- gave RAIO teams air cover

The problem is that we stopped there.

We treated:

- principles as if they were engines,  
- one-shot benchmarks as if they were safety,  
- model-centric risk documents as if they somehow governed **operations**.

From an implementorâ€™s point of view, the next step is not:

- â€œbetter commandmentsâ€, or  
- â€œone more meta-frameworkâ€

Itâ€™s **operational integrity**:

- AI that is evaluated not just on a frontier exam, but on how it behaves in **continuous, self-correcting operation**  
- workflows where **task boundaries** and responsibilities are explicit and reconstructable  
- AI â€œteamsâ€ with **constitutions, not monarchs**  
- rule systems with a **metabolism** â€“ able to change under pressure without collapsing  
- governance that sees both **rules and routes** â€“ the structure of decisions, and the pattern of human lives under them

Thatâ€™s the shift Iâ€™m working on under *Legitimacy Under Acceleration*.

If youâ€™re in a RAIO team, an architecture group, or a policy role and you recognise this gap between **principles and plumbing**, Iâ€™m interested in two things:

- the **incidents and near misses** youâ€™ve seen that current frameworks couldnâ€™t really explain, and  
- the **patterns** youâ€™ve already discovered that make AI systems more fail-safe and more human-coherent in practice.

Thatâ€™s where the next generation of governance will come from: not another set of slogans, but people quietly rebuilding the wiring so that our systems can fail and recover without losing the people theyâ€™re meant to serve.

---

Â© Kelvin Chau, 2025  
This piece is part of the [Legitimacy Under Acceleration](https://github.com/kfkchau/Legitimacy-Under-Acceleration/) project.  
It may be shared non-commercially with attribution but not modified or republished without permission.  
Licensed under [Creative Commons Attributionâ€“NonCommercialâ€“NoDerivatives 4.0 International (CC BY-NC-ND 4.0)](https://creativecommons.org/licenses/by-nc-nd/4.0/)  
For attribution, citation, or inquiries, please refer to:  
ğŸ”— [https://au.linkedin.com/in/kfkchau](https://au.linkedin.com/in/kfkchau)
